net_params:
  transformer: True
  pretrained_emb: False
  attention: False
  ENC_EMB_DIM: 256
  DEC_EMB_DIM: 256
  HID_DIM: 256
  N_LAYERS: 2
  ENC_DROPOUT: 0.1
  DEC_DROPOUT: 0.1
split_ration:
  train_size: 0.8
  valid_size: 0.15
  test_size: 0.05
dataset_path: 'datasets/Machine_translation_EN_RU/data.txt'
BATCH_SIZE: 512
N_EPOCHS: 20
CLIP: 1
model_out_name: transformer_model.pt
lr: 0.001
src_vocab_name: src_vocab_transformer
trg_vocab_name: trg_vocab_transformer
lr_scheduler:
  mode: min
  factor: 0.3
  patience: 4
  threshold: 0.1
  threshold_mode: abs
  cooldown: 0
  min_lr: 1e-5
  eps: 1e-08
